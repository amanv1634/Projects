# Projects
## 1. ECG Classification with CNN
Used numpy , pandas for dataframing seaborn matplotlib for visualization.  Used classification reports to displays the precision, recall, F1, and support scores for the model, In order to support easier interpretation and problem detection. Used to_categorical from keras to do one-hot encoding of the integral frequency data from the dataset to different categories.  Used class_weight feature from sklearn utils to balance the weight of the variables in the dataset. First represented all the categories in the form of a colored plot representing each classification. Used resample from the sklearn to resample the data 4 times so that any inconsistencies gets minimalized. After taking one sample per class out of the resampled data, we plotted it in a chart. Each chart represented a particular classification. To further normalize the data and to make it more continuous, little noise was added to generalize the train dataset. A three layer convolution matrix is used with batch normalization to standardize the inputs of the next layer of the network. Relu (Rectified Linear Units) activation functions are used for each input layer and for the main output layer softmax activation function is used as the number of classifications are more than 2. We use Adam optimization to the model due to the noisy nature of the dataset which will help in handling the sparse gradient of the data.  The model was set to have 40 epochs but the highest accuracy was achieved in the 12th epoch itself, which was 96.69%. It was also worth noticing that the accuracy of the training dataset was more as compared to the validation dataset and the model loss was also a little higher for the training. Nevertheless the model was able to produce a significantly accurate classification. A confusion matrix with normalization was created with classification_Report which showed us that the CNN model of classification had the least collinearity among the classes.

## 2. Sale Price Prediction
This project was mainly done on SaS Enterprise but I did some of my own analysis in python with a sample of the data. There were 81 variables in total which were featuring in the dataset, We wanted to predict the sales price so an exploratory analysis was first done on that variable. On plotting its distribution it was observed that the distribution is deviated right from a normal distribution, which means it is positively skewed. Peakedness(height) of the graph is also good. On plotting the variable sales price with total iving area and the total basement area it was found out that the living area had almost linear relationship with the counterpart. The basement area relationship was also linear but tending more towards an exponential relation. On plotting the overall quality variable with sales price it was observed that with the increasing overall qual the prices also increased, and it was also a significant factor. While computing the correlation between the variables it was also observed that basement squarefootage and 1stfloorsqfootage were so correalted that it was seeming like a multicollinearity problem. Several variables were dropped after conducting a missing data description. Several bivariate analysis were done between the significant variables( saleprice vs grliving area) to observe and remove the outliers between them. The saleprice also showed some peakedness, so a log transformation was applied to normalize the variable. Apparently, the whole dataset was normalized by applying log transformation before the modelling process. Severalss models like Ridge, Lasso, RidgeCV, ElasticNet, LassoCV etc. were tested and compared with the help of crossvalidation_score. The lasso modelling technique had the  lowest root mean squared error score of .12 and it was thus selected. And the GrLiving area was found out to be more significant in depicting the saleprice of a house. Also after this gradient boosting was also tested with XGboost but as the results were similar, it was dropped. 
